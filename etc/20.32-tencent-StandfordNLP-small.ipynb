{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e7af10-0948-45c3-b8ec-376572eb750f",
   "metadata": {},
   "source": [
    "# Building quadrisyll compounds from the Tencent-2M (small) embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835c3638-cd3d-4d96-b892-c19632612877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import tarfile\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from opencc import OpenCC\n",
    "import svgling\n",
    "from stanza.server import CoreNLPClient\n",
    "from stanford_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0ebc8a-d876-4110-851d-ba1693e5f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CORENLP_HOME\"] = os.path.expanduser(\"~/etc/stanford-corenlp-4.4.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4515fbd9-24e4-4b29-8f53-549e76850ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = gzip.open(\"/mnt/md0/seantyh/parsec/tencent-ailab-embedding-zh-d200-v0.2.0-s.tar.gz\", \"r\")\n",
    "tar = tarfile.open(fileobj=fin)\n",
    "_ = tar.next()\n",
    "txt_tzinfo = tar.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47b0e4d-8b5c-481d-8e3c-55cad065c032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tencent-ailab-embedding-zh-d200-v0.2.0-s/tencent-ailab-embedding-zh-d200-v0.2.0-s.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_tzinfo.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f941c9-1353-48b8-a52b-4e1f1db539fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_fobj = tar.extractfile(txt_tzinfo)\n",
    "nvocab, hdim = [int(x) for x in txt_fobj.readline().decode().split(\" \")]\n",
    "nvocab, hdim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893df202-1b4a-4876-b82b-ea9a4c777d0d",
   "metadata": {},
   "source": [
    "## Extract vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6ea28f-781d-4f7d-acae-48c79a299667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096e3c56b2fe4338a4632e1337591fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chpat = re.compile(\"^[\\u3400-\\u4dbf\\u4e00-\\u9fff\\uf900-\\ufaff]+$\")\n",
    "pbar = tqdm(total=nvocab)\n",
    "monosylls = []\n",
    "bisylls = []\n",
    "quads = []\n",
    "embs = []\n",
    "fout = open(\"../data/tencent_vocabs_small.txt\", \"w\")\n",
    "\n",
    "while True:\n",
    "    pbar.update(1)\n",
    "    try:\n",
    "        ln = txt_fobj.readline()\n",
    "        if not ln:\n",
    "            break\n",
    "        toks = ln.decode().strip().split()  \n",
    "        # hvec = np.array([float(x) for x in toks[1:]])\n",
    "        word = toks[0]\n",
    "        if not chpat.match(word):\n",
    "            continue\n",
    "        fout.write(word+\"\\n\")\n",
    "        if len(word)==1:\n",
    "            monosylls.append(word)\n",
    "        elif len(word)==2:\n",
    "            bisylls.append(word)\n",
    "        elif len(word)==4:\n",
    "            quads.append(word)        \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "fout.close()\n",
    "txt_fobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c52ba06-6e8a-4893-b882-9b6c7b165c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649697, 325585, 10818)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quads), len(bisylls), len(monosylls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a046da0d-9047-4a19-86af-39dbb2d97e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f15ccd516849fc9ee3080d5d04c51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/649697 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quads' bisylls constituents are also in vocab 607402\n"
     ]
    }
   ],
   "source": [
    "selected_quads = set()\n",
    "bisylls_set = set(bisylls)\n",
    "\n",
    "for quad_x in tqdm(quads):\n",
    "    c1, c2 = quad_x[:2], quad_x[2:]\n",
    "    if (c1 in bisylls_set and \n",
    "        c2 in bisylls_set):        \n",
    "        selected_quads.add(quad_x)\n",
    "print(\"Quads' bisylls constituents are also in vocab\", len(selected_quads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ee1f87-2563-4161-ace4-1db14846a91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "得罪小人 魔法屏障 光学遥感 航运管理 贫血患者 农业知识 再三追问 粮食种子 募集方式 家电消费\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(islice(selected_quads, 10, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129af2b7-7940-4065-b30c-71981c2743d5",
   "metadata": {},
   "source": [
    "## Parsing a quadrisyllabic word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83fdd9e0-8068-48c4-ad40-cfc3bb9dabbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 06:00:11 INFO: Using CoreNLP default properties for: chinese.  Make sure to have chinese models jar (available for download here: https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH\n",
      "2022-07-06 06:00:11 INFO: Starting server with command: java -Xmx6G -cp /home/seantyh/etc/stanford-corenlp-4.4.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties chinese -annotators tokenize,ssplit,pos,parse -preload -outputFormat serialized\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,64.0,168.0\" width=\"64px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ROOT</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">地理</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">实体</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "TreeLayout(('ROOT', ('NP', ('NN', '地理'), ('NN', '实体'))))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2s = OpenCC('t2s.json')\n",
    "text = t2s.convert(\"地理实体\")\n",
    "with CoreNLPClient(properties=\"chinese\",\n",
    "        annotators=['tokenize','ssplit','pos','parse'],\n",
    "        timeout=30000,\n",
    "        memory='6G', be_quiet=True) as client:\n",
    "    ann = client.annotate(text)\n",
    "svgling.draw_tree(to_linear(ann.sentence[0].parseTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5cf1b10-2c24-4dfb-a11a-3ab5cf6cd766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 06:00:20 INFO: Using CoreNLP default properties for: chinese.  Make sure to have chinese models jar (available for download here: https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH\n",
      "2022-07-06 06:00:20 INFO: Starting server with command: java -Xmx6G -cp /home/seantyh/etc/stanford-corenlp-4.4.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties chinese -annotators tokenize,ssplit,pos,parse -preload -outputFormat serialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12a11b0da5545b28b7258937dd21711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qs_iter = iter(selected_quads)\n",
    "batch_size = 20\n",
    "batch_iter = iter(lambda: list(islice(qs_iter, batch_size)), [])\n",
    "\n",
    "np_compounds = []\n",
    "with CoreNLPClient(properties=\"chinese\",\n",
    "        annotators=['tokenize','ssplit','pos','parse'],\n",
    "        timeout=30000,        \n",
    "        memory='6G', be_quiet=True) as client:    \n",
    "    for qs_list in tqdm(batch_iter):\n",
    "        qs_sen = \"。\".join(x.strip() for x in qs_list)\n",
    "        ann = client.annotate(t2s.convert(qs_sen))\n",
    "        for qs_x, sent_x in zip(qs_list, ann.sentence):\n",
    "            np_nodes = get_nodes(sent_x.parseTree, is_two_bisyll_np)            \n",
    "            if np_nodes:\n",
    "                npcs = flatten_compound(np_nodes[0])\n",
    "                npcs = ((qs_x[:2], qs_x[2:]), npcs[1])\n",
    "                np_compounds.append(npcs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9120290-0c00-4b42-89f8-26bcb9148581",
   "metadata": {},
   "source": [
    "### Write NPs into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4c73d35-9f35-4237-b484-bb47b9205d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_path = \"../data/tencent_vocab_nps_small.txt\"\n",
    "with open(compound_path, \"w\", encoding=\"UTF-8\") as fout:\n",
    "    for np_x in np_compounds:\n",
    "        nn_x = np_x[0]\n",
    "        fout.write(\"{},{},{}\\n\".format(''.join(nn_x), nn_x[0], nn_x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22f5ff5f-8cb3-4ae9-bd59-914ecb2216bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_path = \"../data/tencent_vocab_nn_compounds_small.txt\"\n",
    "nn_compounds = []\n",
    "with open(nn_path, \"w\", encoding=\"UTF-8\") as fout:\n",
    "    for np_x in np_compounds:        \n",
    "        nn_x = np_x[0]\n",
    "        pos_x = np_x[1]\n",
    "        if any(x!=\"NN\" for x in pos_x) or \\\n",
    "           any(len(w)!=2 for w in nn_x):\n",
    "            continue\n",
    "        fout.write(\"{},{},{}\\n\".format(''.join(nn_x), nn_x[0], nn_x[1]))\n",
    "        nn_compounds.append(nn_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e4f9eed-1035-48a1-a140-8fd289ff8ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192882"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nn_compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "367aa6b0-1865-436e-bfa3-d5f29469cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['旅客需求', '巡逻时间', '魔法屏障', '光学遥感', '航运管理']\n"
     ]
    }
   ],
   "source": [
    "print([''.join(x) for x in nn_compounds[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6d534-397b-4e72-97d5-a2a328278f2e",
   "metadata": {},
   "source": [
    "## Collect words to include in output embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "275b1289-c22d-480e-8d44-5855e311af73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b619af83fe8449b38debf86ce2fc1d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192882 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected words 231601\n"
     ]
    }
   ],
   "source": [
    "selected = set()\n",
    "bisylls_set = set(bisylls)\n",
    "filtered = []\n",
    "\n",
    "for nn_x in tqdm(nn_compounds):\n",
    "    c1, c2 = nn_x\n",
    "    if (c1 in bisylls_set and \n",
    "        c2 in bisylls_set):        \n",
    "        selected.update((c1, c2, ''.join(nn_x)))\n",
    "    else:\n",
    "        filtered.append(''.join(nn_x))\n",
    "selected.update(monosylls)\n",
    "print(\"selected words\", len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "955c655b-f1f3-4b00-b2bd-985049534a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this should be an empty list\n",
    "len(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1386d6c6-c800-409d-9a7c-da93ee4e6d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 10818, 2: 27901, 4: 192882}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "{gk: len(list(gv)) \n",
    " for gk, gv \n",
    " in groupby(sorted(selected, key=len), key=len)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c56ecc-6500-4eab-a745-93e88d20a6f1",
   "metadata": {},
   "source": [
    "## Subsetting the original embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aa605da-4568-401a-840a-740781d4720a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0511009575746d8b0ae29f58341b33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231602  written (include header)\n"
     ]
    }
   ],
   "source": [
    "txt_fobj = tar.extractfile(txt_tzinfo)\n",
    "nvocab, hdim = [int(x) for x in txt_fobj.readline().decode().split(\" \")]\n",
    "nvocab, hdim\n",
    "out_nvocab = len(selected)\n",
    "sub_emb_path = \"../data/tencent_nn_embeddings-s.txt.gz\"\n",
    "fout = gzip.open(sub_emb_path, \"wb\")\n",
    "fout.write(f\"{out_nvocab} {hdim}\\n\".encode())\n",
    "pbar = tqdm(total=nvocab)\n",
    "fout_lines = 1\n",
    "out_buffer = selected.copy()\n",
    "\n",
    "while True:\n",
    "    pbar.update(1)\n",
    "    try:\n",
    "        ln = txt_fobj.readline()\n",
    "        if not ln:\n",
    "            break\n",
    "        ln = ln.decode()\n",
    "        toks = ln.strip().split()  \n",
    "        # hvec = np.array([float(x) for x in toks[1:]])\n",
    "        word = toks[0]\n",
    "        if word in selected:\n",
    "            out_buffer.remove(word)\n",
    "            fout_lines += 1\n",
    "            fout.write(ln.encode())        \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "pbar.close()\n",
    "fout.close()\n",
    "txt_fobj.close()\n",
    "print(fout_lines, \" written (include header)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98857b75-648c-4a6f-a02d-1b3e3b64e536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check whether there is any word left in the `selected`\n",
    "## `out_buffer` should be an empty set\n",
    "out_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb5e10-1345-47d2-9be7-c7dfa2f674e0",
   "metadata": {},
   "source": [
    "## Test embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbda6ecf-0ee1-4bee-800b-131d7a62b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c927a56-62d6-4fed-b437-24c954960e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = KeyedVectors.load_word2vec_format(sub_emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "877b99d1-0910-4f41-9a25-8e380e9b8273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231601"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e10f0f0f-afc5-456e-b3db-a1a2fdbc08d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('英文字幕', 0.8481001853942871),\n",
       " ('字幕', 0.84283846616745),\n",
       " ('日语字幕', 0.8051227331161499)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.most_similar(\"中文字幕\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c007da4-de33-4392-945b-9a9a4a4daf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('英文', 0.9023275971412659),\n",
       " ('日文', 0.851484477519989),\n",
       " ('韩文', 0.8410767912864685)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.most_similar(\"中文\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d1094aa-cdbf-4cfe-b059-dfea0d0e4400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('偏旁', 0.7554795145988464),\n",
       " ('汉字', 0.7427154183387756),\n",
       " ('繁体', 0.7201398611068726)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.most_similar(\"字\", topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa4ba5-fff9-42bb-8461-b30b61d1271a",
   "metadata": {},
   "source": [
    "## Output hashes\n",
    "```\n",
    "../data/tencent_vocab_nps_small.txt 8662a8\n",
    "../data/tencent_vocab_nn_compounds_small.txt a29857\n",
    "../data/tencent_nn_embeddings-s.txt.gz b237d2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14e9406d-8ea6-4292-a9bc-83bf478bc61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tencent_vocab_nps_small.txt 8662a8\n",
      "../data/tencent_vocab_nn_compounds_small.txt a29857\n",
      "../data/tencent_nn_embeddings-s.txt.gz b237d2\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from pathlib import Path\n",
    "for path_x in (compound_path, nn_path, sub_emb_path):\n",
    "    h = hashlib.sha1()\n",
    "    h.update(Path(path_x).read_bytes())\n",
    "    print(path_x, h.hexdigest()[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e30083-2f4f-4bd0-bd0c-50081e0825e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

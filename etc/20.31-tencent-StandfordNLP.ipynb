{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835c3638-cd3d-4d96-b892-c19632612877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import tarfile\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from opencc import OpenCC\n",
    "from stanza.server import CoreNLPClient\n",
    "from stanford_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0ebc8a-d876-4110-851d-ba1693e5f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CORENLP_HOME\"] = os.path.expanduser(\"~/etc/stanford-corenlp-4.4.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4515fbd9-24e4-4b29-8f53-549e76850ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = gzip.open(\"/mnt/md0/seantyh/parsec/tencent-ailab-embedding-zh-d200-v0.2.0.tar.gz\", \"r\")\n",
    "tar = tarfile.open(fileobj=fin)\n",
    "_ = tar.next()\n",
    "txt_tzinfo = tar.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47b0e4d-8b5c-481d-8e3c-55cad065c032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tencent-ailab-embedding-zh-d200-v0.2.0/tencent-ailab-embedding-zh-d200-v0.2.0.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_tzinfo.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f941c9-1353-48b8-a52b-4e1f1db539fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12287936, 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_fobj = tar.extractfile(txt_tzinfo)\n",
    "nvocab, hdim = [int(x) for x in txt_fobj.readline().decode().split(\" \")]\n",
    "nvocab, hdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6ea28f-781d-4f7d-acae-48c79a299667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7db31aa86d2494d8abc81afc99688bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12287936 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## write out vocabulary\n",
    "chpat = re.compile(\"^[\\u3400-\\u4dbf\\u4e00-\\u9fff\\uf900-\\ufaff]+$\")\n",
    "pbar = tqdm(toupdatel=nvocab)\n",
    "monosylls = []\n",
    "bisylls = []\n",
    "quads = []\n",
    "embs = []\n",
    "fout = open(\"../data/tencent_vocabs.txt\", \"w\")\n",
    "\n",
    "while True:\n",
    "    pbar.update(1)\n",
    "    try:\n",
    "        ln = txt_fobj.readline()\n",
    "        if not ln:\n",
    "            break\n",
    "        toks = ln.decode().strip().split()  \n",
    "        # hvec = np.array([float(x) for x in toks[1:]])\n",
    "        word = toks[0]\n",
    "        if not chpat.match(word):\n",
    "            continue\n",
    "        fout.write(word+\"\\n\")\n",
    "        if len(word)==1:\n",
    "            monosylls.append(word)\n",
    "        elif len(word)==2:\n",
    "            bisylls.append(word)\n",
    "        elif len(word)==4:\n",
    "            quads.append(word)        \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c52ba06-6e8a-4893-b882-9b6c7b165c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2815445, 1675801, 21283)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quads), len(bisylls), len(monosylls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ee1f87-2563-4161-ace4-1db14846a91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在线观看 中文字幕 在线视频 免费观看 在线播放 影音先锋 有限公司 高清无码 免费视频 久久综合\n",
      "大和物語 武元唯衣 松田里奈 生活橱窗 都卜林格 柏树街站 阿宾斯克 天童如净 鲁亨盖里 教宗御座\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(quads[:10]))\n",
    "print(\" \".join(quads[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346193d-7948-462d-84ab-1b717e313d90",
   "metadata": {},
   "source": [
    "## Parse quads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5cf1b10-2c24-4dfb-a11a-3ab5cf6cd766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 06:21:54 INFO: Using CoreNLP default properties for: chinese.  Make sure to have chinese models jar (available for download here: https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH\n",
      "2022-07-05 06:21:54 INFO: Starting server with command: java -Xmx6G -cp /home/seantyh/etc/stanford-corenlp-4.4.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties chinese -annotators tokenize,ssplit,pos,parse -preload -outputFormat serialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dc40b95c9f423fbecfb254f17f0be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qs_iter = iter(quads)\n",
    "batch_size = 20\n",
    "batch_iter = iter(lambda: list(islice(qs_iter, batch_size)), [])\n",
    "t2s = OpenCC('t2s.json')\n",
    "\n",
    "np_compounds = []\n",
    "with CoreNLPClient(properties=\"chinese\",\n",
    "        annotators=['tokenize','ssplit','pos','parse'],\n",
    "        timeout=30000,        \n",
    "        memory='6G', be_quiet=True) as client:    \n",
    "    for qs_list in tqdm(batch_iter):\n",
    "        qs_sen = \"。\".join(x.strip() for x in qs_list)\n",
    "        ann = client.annotate(t2s.convert(qs_sen))\n",
    "        for qs_x, sent_x in zip(qs_list, ann.sentence):\n",
    "            np_nodes = get_nodes(sent_x.parseTree, is_two_bisyll_np)            \n",
    "            if np_nodes:\n",
    "                npcs = flatten_compound(np_nodes[0])\n",
    "                np_compounds.append(npcs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a89bc11-9099-43e6-bdf7-f83817bcba45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('中文', '字幕'), ('NN', 'NN')],\n",
       " [('在线', '视频'), ('JJ', 'NN')],\n",
       " [('影音', '先锋'), ('NN', 'NN')],\n",
       " [('有限', '公司'), ('JJ', 'NN')],\n",
       " [('这个', '时候'), ('DT', 'NN')],\n",
       " [('奇米', '影视'), ('NR', 'NN')],\n",
       " [('当前', '位置'), ('NT', 'NN')],\n",
       " [('国产', '精品'), ('JJ', 'NN')],\n",
       " [('在线', '影院'), ('JJ', 'NN')],\n",
       " [('视频', '在线'), ('NN', 'NN')]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_compounds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4c73d35-9f35-4237-b484-bb47b9205d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_path = \"../data/tencent_vocab_nps.txt\"\n",
    "with open(compound_path, \"w\", encoding=\"UTF-8\") as fout:\n",
    "    for np_x in np_compounds:\n",
    "        nn_x = np_x[0]\n",
    "        fout.write(\"{},{},{}\\n\".format(''.join(nn_x), nn_x[0], nn_x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22f5ff5f-8cb3-4ae9-bd59-914ecb2216bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_path = \"../data/tencent_vocab_nn_compounds.txt\"\n",
    "with open(compound_path, \"w\", encoding=\"UTF-8\") as fout:\n",
    "    for np_x in np_compounds:        \n",
    "        nn_x = np_x[0]\n",
    "        pos_x = np_x[1]\n",
    "        if any(x!=\"NN\" for x in pos_x) or \\\n",
    "           any(len(w)!=2 for w in nn_x):\n",
    "            continue\n",
    "        fout.write(\"{},{},{}\\n\".format(''.join(nn_x), nn_x[0], nn_x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14e9406d-8ea6-4292-a9bc-83bf478bc61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tencent_vocab_nps.txt f4ac41\n",
      "../data/tencent_vocab_nn_compounds.txt 67402c\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from pathlib import Path\n",
    "for path_x in (compound_path, nn_path):\n",
    "    h = hashlib.sha1()\n",
    "    h.update(Path(path_x).read_bytes())\n",
    "    print(path_x, h.hexdigest()[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e30083-2f4f-4bd0-bd0c-50081e0825e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
